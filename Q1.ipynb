{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#mounting the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayAHEz2nJWje",
        "outputId": "42f42cdc-4d0f-4974-ae4d-d61025be4d12"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing header files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "9_5vrP_ZTOh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "581bdc96-17aa-4a98-df84-4b1f8298aed9"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################################### PART A #############################################\n",
        "#convert text to lowercase\n",
        "def convert_lower_case(text):\n",
        "  return np.char.lower(text)\n",
        "\n",
        "#remove underscore\n",
        "def rem_underscore(text):\n",
        "  t=str(text)\n",
        "  return t.replace(\"_\",\"\")\n",
        "\n",
        "#Remove stopwords from text\n",
        "def remove_stop_words(text):  \n",
        "    stop_words = stopwords.words('english')\n",
        "    token = word_tokenize(str(text))\n",
        "    text2 = \"\"\n",
        "    for t in token:\n",
        "      if t not in stop_words:\n",
        "        text2 = text2+\" \"+t\n",
        "    return np.char.strip(text2)\n",
        "\n",
        "#Remove punctuation marks from text\n",
        "def remove_punctuation(text):   \n",
        "    symbols = \"/:;!\\\"#+-.&()*^_`<=>?@[\\                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ]$%{|}~\\n\"\n",
        "    for i in range(len(symbols)):\n",
        "        text = np.char.replace(text, symbols[i], ' ')\n",
        "        text = np.char.replace(text, \"  \", \" \")\n",
        "    text = np.char.replace(text, ',', ' ')\n",
        "    text = np.char.replace(text, \"'\", \"\")\n",
        "    return text\n",
        "\n",
        "#Remove blank space\n",
        "def remove_blank_space(text):\n",
        "  t=str(text)\n",
        "  return \" \".join(t.split())\n",
        "\n",
        "#Remove single characters\n",
        "def remove_single_char(text):\n",
        "    words = word_tokenize(str(text))\n",
        "    text2 = \"\"\n",
        "    for w in words:\n",
        "        if len(w) > 1:\n",
        "            text2 = text2 + \" \" + w\n",
        "    return np.char.strip(text2)  \n",
        "\n",
        "#convert numbers to text\n",
        "def convert_number_to_text(text):\n",
        "    text = np.char.replace(text, \"0\", \" zero \")     #replace 0 by zero in text.\n",
        "    text = np.char.replace(text, \"1\", \" one \")      #replace 1 by one in text.\n",
        "    text = np.char.replace(text, \"2\", \" two \")      #replace 2 by two in textd.\n",
        "    text = np.char.replace(text, \"3\", \" three \")    #replace 3 by three in text.\n",
        "    text = np.char.replace(text, \"4\", \" four \")     #replace 4 by four in text.\n",
        "    text = np.char.replace(text, \"5\", \" five \")     #replace 5 by five in text.\n",
        "    text = np.char.replace(text, \"6\", \" six \")      #replace 6 by six in text.\n",
        "    text = np.char.replace(text, \"7\", \" seven \")    #replace 7 by seven in text.\n",
        "    text = np.char.replace(text, \"8\", \" eight \")    #replace 8 by eight in text.\n",
        "    text = np.char.replace(text, \"9\", \" nine \")     #replace 9 by nine in text.\n",
        "    return text\n",
        "\n",
        "#performing lemmatization on data\n",
        "def lem(text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = word_tokenize(str(text))\n",
        "  text2 = \"\"\n",
        "  for w in tokens:\n",
        "      text2 = text2 + \" \" + lemmatizer.lemmatize(w)\n",
        "  return np.char.strip(text2)\n",
        "\n",
        "#removing non-ascii characters\n",
        "def remove_non_ascii(text):\n",
        "  t=str(text)\n",
        "  return t.encode(\"ascii\",\"ignore\").decode(\"utf-8\",\"ignore\")\n",
        "\n",
        "#perform preprocessing\n",
        "def preprocess_data(text):  \n",
        "  text = convert_lower_case(text)\n",
        "  text = rem_underscore(text)\n",
        "  text = convert_number_to_text(text)\n",
        "  text = remove_punctuation(text)\n",
        "  text = remove_stop_words(text)\n",
        "  text = remove_blank_space(text)\n",
        "  text = remove_single_char(text)\n",
        "  text = remove_non_ascii(text)\n",
        "  text = lem(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "Q_rtCx0tvTzz"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################## PART B ############################################\n",
        "def create_inverted_index(paths):     #function create inverted index.\n",
        "  store_filename = []                 #list for storing all file names\n",
        "  docID = 0                           #counter for documents\n",
        "  tii2=dict()                         # dictionary for storing index with word as key and position as values\n",
        "\n",
        "  #loop for calculating index for each file \n",
        "  for path in paths: \n",
        "    file = open(path, 'r', encoding= 'ISO-8859-1')    \n",
        "    text = \" \".join(file.read().split())    # for removing extra spaces\n",
        "    file.close()\n",
        "    print(docID)\n",
        "    preprocessed_data = preprocess_data(text)\n",
        "    tokens = word_tokenize(str(preprocessed_data))      #tokenize the preprocessed text and convert into tokens\n",
        "      \n",
        "    #loop for storing data in dictionary\n",
        "    for t in tokens:\n",
        "      if t not in tii2.keys():\n",
        "        tii2[t] = list()\n",
        "        tii2[t].append(docID)\n",
        "      else:\n",
        "        if docID not in tii2[t]:\n",
        "          tii2[t].append(docID)\n",
        "\n",
        "    filename = os.path.basename(path)        #extract last filename from  the path\n",
        "    store_filename.append([filename])        #append into list\n",
        "    docID += 1\n",
        "    \n",
        "  store_filename = pd.DataFrame(store_filename)     #convert filenames list of list into dataframe\n",
        "\n",
        "  # save the inverted index and filenames in pickle file so that can we build futher\n",
        "  with open('fn.pickle', 'wb') as handle:\n",
        "    pickle.dump(tii2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  store_filename.to_pickle(\"sf\")"
      ],
      "metadata": {
        "id": "N4RLnMn2T_FW"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################### PART C #######################################################\n",
        "def not_operation(var):                      #function to perform not operation\n",
        "  v = set(range(len(paths)))\n",
        "  return list(v.difference(var))\n",
        "\n",
        "def or_operation(var1,var2):                 #function to perform or operation\n",
        "  n = len(var1)                              #store length of var1 and var2 into n and m.\n",
        "  m = len(var2)\n",
        "  Result = []\n",
        "  comparisons = 0\n",
        "  i = 0\n",
        "  j = 0\n",
        "\n",
        "  while i<n and j<m:            #perform union in two set X and Y and count number of comparision.\n",
        "    if var1[i]==var2[j]:\n",
        "      Result.append(var1[i])\n",
        "      i=i+1\n",
        "      j=j+1\n",
        "      comparisons=comparisons+1\n",
        "    elif var1[i]<var2[j]:\n",
        "      Result.append(var1[i])\n",
        "      i=i+1\n",
        "      comparisons=comparisons+1\n",
        "    else:\n",
        "      Result.append(var2[j])\n",
        "      j=j+1\n",
        "      comparisons=comparisons+1 \n",
        "\n",
        "  while i<n:                      #when only var1 set left then append into result without any comparision.\n",
        "    Result.append(var1[i])\n",
        "    i=i+1\n",
        "\n",
        "  while j<m:                      #when only var2 set left then append into result without any comparision.\n",
        "    Result.append(var2[j])\n",
        "    j=j+1  \n",
        "\n",
        "  return Result, comparisons           #return union of var1 and var2 and number of comparision.\n",
        "\n",
        "def and_operation(var1,var2):        #function to perform and operation\n",
        "  n = len(var1)                      #store length of var1 and var2 into n and m.\n",
        "  m = len(var2)\n",
        "  Result = []\n",
        "  comparisons =0\n",
        "  i = 0\n",
        "  j = 0\n",
        "\n",
        "  while i<n and j<m:           #perform union in two set var1 and var2 and count number of comparision.\n",
        "    if var1[i]==var2[j]:\n",
        "      Result.append(var1[i])\n",
        "      i=i+1\n",
        "      j=j+1\n",
        "      comparisons=comparisons+1\n",
        "    elif var1[i]<var2[j]:\n",
        "      i=i+1\n",
        "      comparisons=comparisons+1\n",
        "    else:\n",
        "      j=j+1\n",
        "      comparisons=comparisons+1\n",
        "  return Result, comparisons     #return intersection of var1 and var2 and number of comparision.\n",
        "\n",
        "def or_not_operation(var1, var2):                    #function to perform or Not operation\n",
        "    var2 = not_operation(var2)                       #find not of var2.\n",
        "    Result, comparisons = or_operation(var1, var2)   #after finding not of var2 then perform OR of var1 and var2 and find no of comparision.\n",
        "    return Result, comparisons                       #return the result after var1 or Not var2 and number of comparision.\n",
        "  \n",
        "def and_not_operation(var1, var2):                   #function to perform and not operation\n",
        "    var2 = not_operation(var2)                       #find not of var2.\n",
        "    Result, comparisons = and_operation(var1, var2)  #after finding not of Y then perform and of var1 and var2 and find number of comparision.\n",
        "    return Result, comparisons                       #return the result after var1 or Not var2 and number of comparision."
      ],
      "metadata": {
        "id": "-Sj3pAgsZssh"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_result(Query_docIds):\n",
        "  op_pos = 0                        #iterator for positions\n",
        "  Total_comp = 0                    #to store total comaprisons\n",
        "  X = Query_docIds[0]\n",
        "  for operand in Query_docIds[1:]:\n",
        "    Y = operand\n",
        "\n",
        "    operator = operations[op_pos]\n",
        "    op_pos = op_pos+1\n",
        "\n",
        "    if(operator == 'OR'):      #if operator is OR then call Function_or and find resultant list and number of comparision.\n",
        "      X, No_of_compare = or_operation(X, Y)\n",
        "      Total_comp = Total_comp + No_of_compare\n",
        "\n",
        "    elif(operator == 'AND'):       #if operator is AND then call Function_or and find resultant list and number of comparision.\n",
        "      X, No_of_compare = and_operation(X, Y)\n",
        "      Total_comp = Total_comp + No_of_compare\n",
        "\n",
        "    elif(operator == 'OR NOT'):       #if operator is OR NOT then call Function_or and find resultant list and number of comparision.\n",
        "      X, No_of_compare = or_not_operation(X, Y)\n",
        "      Total_comp = Total_comp + No_of_compare\n",
        "\n",
        "    elif(operator == 'AND NOT'):      #if operator is AND NOT then call Function_or and find resultant list and number of comparision.\n",
        "      X, No_of_compare = and_not_operation(X, Y)\n",
        "      Total_comp = Total_comp + No_of_compare\n",
        "\n",
        "    else:\n",
        "      print(\"You Entered Wrong Operations................!!!\")\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"Number of documents matched: \", len(X))\n",
        "  print(\"No. of comparisons required: \",Total_comp)  \n",
        "\n",
        "  # print the document names retrieved\n",
        "  name_list = []\n",
        "  for i in X:\n",
        "    name_list.append(store_filename.iloc[i][0])\n",
        "  print(\"Documents retreived are:-\")\n",
        "  print(name_list)"
      ],
      "metadata": {
        "id": "bvunVIML7P_J"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  files_path = \"/content/drive/MyDrive/Humor,Hist,Media,Food\"\n",
        "\n",
        "  # find all files those are stored into files_path and store into path.\n",
        "  paths = []\n",
        "  for (dirpath, dirnames, filenames) in os.walk(str(files_path)):\n",
        "    for i in filenames:\n",
        "       paths.append(str(dirpath)+str(\"/\")+i)\n",
        "  \n",
        "  create_inverted_index(paths)    #for creating index\n",
        "  store_filename = []             #for storing filenames\n",
        "\n",
        "  # read the index and filename \n",
        "  with open('fn.pickle', 'rb') as handle:\n",
        "    inverted_index = pickle.load(handle)\n",
        "  store_filename=pd.read_pickle(\"sf\")\n",
        "\n",
        "  # Our input query\n",
        "  N = int(input(\"Enter Number of Queries you want to test:-\"))  \n",
        "  while N>0:\n",
        "    Query = input(\"Enter the query:-\")\n",
        "    operations  = input(\"Enter operations with comma between them:-\")\n",
        "    operations=operations.upper()\n",
        "    operations=operations.split(\",\")\n",
        "    operations = [i.strip() for i in operations]\n",
        "\n",
        "  # Query Pre-processing\n",
        "    pre_processed_query = preprocess_data(Query)\n",
        "    pre_processed_query = pre_processed_query.flatten()\n",
        "    pre_processed_query = np.char.split(pre_processed_query[0])\n",
        "    pre_processed_query = pre_processed_query.tolist()\n",
        "\n",
        "  #add word documents id to query document ids in sorted order\n",
        "    Query_docIds = []\n",
        "\n",
        "    for r in pre_processed_query:\n",
        "      if r not in inverted_index.keys():\n",
        "        print(r+\" is not in any document. Please enter valid query\")\n",
        "        break\n",
        "      word_docIds = inverted_index[r]\n",
        "      Query_docIds.append(sorted(word_docIds))\n",
        "\n",
        "    #check if processed query documents are 1 less than number of operations \n",
        "    if(len(Query_docIds)-1 == len(operations)):\n",
        "      cal_result(Query_docIds)  \n",
        "    else:\n",
        "      print(\"Wrong Input\")\n",
        "    N-=1"
      ],
      "metadata": {
        "id": "AIkA7KcDRPOP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}