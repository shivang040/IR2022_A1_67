{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ql6vuTZ0H78z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJCz5yelTT8o",
        "outputId": "dc9729bc-a7f3-49b8-f709-973797524c8f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MPQ3UuMj6nGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013d3e8a-bc69-4a72-e325-82c2ecb13e0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "nltk.download('stopwords')  # download stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qC8F2_87pta",
        "outputId": "9976816f-a4e4-4e0d-9200-80670130dcd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1133\n"
          ]
        }
      ],
      "source": [
        "files_path = \"/content/drive/MyDrive/Humor,Hist,Media,Food\"\n",
        "\n",
        "#Store path of all files in paths\n",
        "paths = []\n",
        "for (dirpath, dirnames, filenames) in os.walk(str(files_path)):\n",
        "  for i in filenames:\n",
        "     paths.append(str(dirpath)+str(\"/\")+i)\n",
        "\n",
        "print(len(paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Hmh5XrlhZlAZ"
      },
      "outputs": [],
      "source": [
        "######################################   part a   ############################################\n",
        "\n",
        "#Convert the text to lower case\n",
        "def convert_lower_case(text):   \n",
        "    return np.char.lower(text)\n",
        "\n",
        "#Perform word tokenization\n",
        "def word_tokenization(text):\n",
        "  tokens = word_tokenize(str(text))\n",
        "  return tokens\n",
        "\n",
        "#Remove stopwords from tokens\n",
        "def remove_stop_words(tokens):  \n",
        "    stop_words = stopwords.words('english')\n",
        "    filtered_tokens = []\n",
        "    for word in tokens:\n",
        "        if word not in stop_words:\n",
        "            filtered_tokens.append(word)\n",
        "    return filtered_tokens\n",
        "\n",
        "#Remove punctuation marks from tokens\n",
        "def remove_punctuation(words):   \n",
        "    filtered_words = []\n",
        "    for word in words:\n",
        "        filtered_word = re.sub(r'[^\\w\\s]', ' ', word)    #\\w for string containing char a-z,A-Z,0-9,_, \\s for whitespaces, ^for except these \n",
        "        if filtered_word != '':\n",
        "            filtered_words.append(filtered_word)\n",
        "    return filtered_words\n",
        "\n",
        "#Remove blank space tokens\n",
        "def remove_blank_space_tokens(words):\n",
        "  words=' '.join(words).split()\n",
        "  return words \n",
        "\n",
        "#Perform preprocessing\n",
        "def preprocess_data(text):\n",
        "    text = convert_lower_case(text)\n",
        "    tokens = word_tokenization(text)\n",
        "    filtered_tokens = remove_stop_words(tokens)\n",
        "    filtered_tokens = remove_punctuation(filtered_tokens)\n",
        "    filtered_tokens = remove_blank_space_tokens(filtered_tokens)\n",
        "    return filtered_tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################################   part b   ############################################\n",
        "\n",
        "def find_pos_of_list_having_doc(posting_list, doc):\n",
        "    pos = 0\n",
        "    for a in posting_list:\n",
        "        if a[0] == doc:     \n",
        "            return pos         #return position of list in postings having doc at 0th position\n",
        "        pos+=1\n",
        "    return \"\"\n",
        "\n",
        "doc_no = 0\n",
        "positional_index = {}\n",
        "doc_map = {}\n",
        "\n",
        "for path in paths:\n",
        "    file = open(path, 'r', encoding='ISO-8859-1')\n",
        "    text = file.read().strip()\n",
        "    file.close()\n",
        "    tokens = preprocess_data(text)\n",
        " \n",
        "    for position, term in enumerate(tokens):\n",
        "        if term in positional_index : \n",
        "            positional_index[term][1] += 1\n",
        "            posting = positional_index[term][0]                    #posting list of token  \n",
        "            pos = find_pos_of_list_having_doc(posting,doc_no)   \n",
        "            if pos != \"\" :                                         #if doc already added\n",
        "                positional_index[term][0][pos][1].add(position)    #add position to set\n",
        "            else:                                                  #if doc added first time \n",
        "                positional_index[term][0].append([])               #create new list \n",
        "                positional_index[term][0][-1].append(doc_no)       #append doc_no to the newly appended list(-1 is position of new list)\n",
        "                positional_index[term][0][-1].append(set())        #append set for positions to the newly appended list\n",
        "                positional_index[term][0][-1][1].add(position)     #add position to this newly appended set\n",
        "        else:\n",
        "            positional_index[term] = []                            #new list as a value for token\n",
        "            positional_index[term].append([])                      #list at 1st pos for storing posting lists for each doc\n",
        "            positional_index[term].append(1)                       #occurence count of token at 2nd position\n",
        "            positional_index[term][0].append([])                   #create new list \n",
        "            positional_index[term][0][-1].append(doc_no)           #append doc_no to the newly appended list(-1 is position of new list)\n",
        "            positional_index[term][0][-1].append(set())            #append set for positions to the newly appended list\n",
        "            positional_index[term][0][-1][1].add(position)         #add position to this newly appended set\n",
        "    filename = os.path.basename(path)\n",
        "    doc_map[doc_no] = filename\n",
        "    doc_no+=1\n",
        "    \n",
        "df = pd.DataFrame(positional_index)\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "Fzdp5x_tw1g4",
        "outputId": "987e6f91-f6ef-404e-9ec3-b51e8a824d05"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ea00b9e5-285a-4eb8-961e-c8d39bdd47d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>newsgroups</th>\n",
              "      <th>talk</th>\n",
              "      <th>bizarre</th>\n",
              "      <th>rigler</th>\n",
              "      <th>dao</th>\n",
              "      <th>nrc</th>\n",
              "      <th>ca</th>\n",
              "      <th>michael</th>\n",
              "      <th>subject</th>\n",
              "      <th>t</th>\n",
              "      <th>b</th>\n",
              "      <th>boxed</th>\n",
              "      <th>edition</th>\n",
              "      <th>message</th>\n",
              "      <th>id</th>\n",
              "      <th>1992dec11</th>\n",
              "      <th>033233</th>\n",
              "      <th>26164</th>\n",
              "      <th>sol</th>\n",
              "      <th>uvic</th>\n",
              "      <th>reply</th>\n",
              "      <th>to</th>\n",
              "      <th>organization</th>\n",
              "      <th>cadc</th>\n",
              "      <th>date</th>\n",
              "      <th>fri</th>\n",
              "      <th>11</th>\n",
              "      <th>dec</th>\n",
              "      <th>92</th>\n",
              "      <th>03</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>gmt</th>\n",
              "      <th>lines</th>\n",
              "      <th>372</th>\n",
              "      <th>board</th>\n",
              "      <th>game</th>\n",
              "      <th>copyright</th>\n",
              "      <th>1992</th>\n",
              "      <th>labs</th>\n",
              "      <th>...</th>\n",
              "      <th>couters</th>\n",
              "      <th>couterfeiting</th>\n",
              "      <th>niven</th>\n",
              "      <th>10megz</th>\n",
              "      <th>06601030305800</th>\n",
              "      <th>f0110030</th>\n",
              "      <th>kvm</th>\n",
              "      <th>stacc</th>\n",
              "      <th>halk</th>\n",
              "      <th>cided</th>\n",
              "      <th>ungrateful</th>\n",
              "      <th>cin</th>\n",
              "      <th>acheing</th>\n",
              "      <th>recommande</th>\n",
              "      <th>rb</th>\n",
              "      <th>natuurlijk</th>\n",
              "      <th>nemen</th>\n",
              "      <th>goeie</th>\n",
              "      <th>belgische</th>\n",
              "      <th>yeeaaah</th>\n",
              "      <th>dispensaries</th>\n",
              "      <th>samurais</th>\n",
              "      <th>dogfood</th>\n",
              "      <th>sprit</th>\n",
              "      <th>practises</th>\n",
              "      <th>bodys</th>\n",
              "      <th>hairdryer</th>\n",
              "      <th>yeeaaahhhhh</th>\n",
              "      <th>wechselstr</th>\n",
              "      <th>lotsa</th>\n",
              "      <th>niki</th>\n",
              "      <th>mmmhh</th>\n",
              "      <th>halfwit</th>\n",
              "      <th>butterman</th>\n",
              "      <th>snotface</th>\n",
              "      <th>robosig</th>\n",
              "      <th>errorfree</th>\n",
              "      <th>sigmaker</th>\n",
              "      <th>dowdy</th>\n",
              "      <th>laver</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[0, {0, 1416}], [15, {0, 2039}], [23, {0, 232...</td>\n",
              "      <td>[[0, {1, 42, 103}], [3, {374}], [6, {2667, 101...</td>\n",
              "      <td>[[0, {104, 2, 43}], [4, {349, 53}], [15, {1715...</td>\n",
              "      <td>[[0, {1121, 3, 8, 48, 1588, 1589, 24}]]</td>\n",
              "      <td>[[0, {25, 4, 29, 1590}], [639, {1530}], [648, ...</td>\n",
              "      <td>[[0, {26, 5, 1591}], [450, {6594, 6598}], [593...</td>\n",
              "      <td>[[0, {1592, 27, 21, 6}], [1, {272, 97, 105}], ...</td>\n",
              "      <td>[[0, {1586, 7}], [17, {396}], [25, {645}], [29...</td>\n",
              "      <td>[[0, {9}], [10, {6}], [15, {129, 9, 4434, 4793...</td>\n",
              "      <td>[[0, {10, 1422, 145, 177, 1266, 212, 1298, 120...</td>\n",
              "      <td>[[0, {96, 1152, 11, 178, 146, 1267, 213, 1299,...</td>\n",
              "      <td>[[0, {171, 12}], [968, {668}]]</td>\n",
              "      <td>[[0, {13}], [12, {73}], [13, {255}], [39, {325...</td>\n",
              "      <td>[[0, {496, 14}], [15, {5377, 899, 4553, 16, 44...</td>\n",
              "      <td>[[0, {15}], [15, {17, 92, 118}], [23, {17}], [...</td>\n",
              "      <td>[[0, {16}]]</td>\n",
              "      <td>[[0, {17}]]</td>\n",
              "      <td>[[0, {18}]]</td>\n",
              "      <td>[[0, {19}], [23, {1795}], [210, {10}], [286, {...</td>\n",
              "      <td>[[0, {20}], [84, {10206}], [100, {716}], [843,...</td>\n",
              "      <td>[[0, {22}], [10, {531}], [22, {1360}], [25, {8...</td>\n",
              "      <td>[[0, {23}], [28, {22}], [35, {3560, 3534, 1233...</td>\n",
              "      <td>[[0, {28}], [15, {23}], [18, {281}], [23, {23}...</td>\n",
              "      <td>[[0, {30}]]</td>\n",
              "      <td>[[0, {31}], [10, {0}], [15, {2440, 3211, 2324,...</td>\n",
              "      <td>[[0, {32}], [39, {4}], [50, {761, 1299, 2020}]...</td>\n",
              "      <td>[[0, {33}], [1, {174}], [7, {93, 87}], [10, {2...</td>\n",
              "      <td>[[0, {34}], [15, {4463, 4850, 4211, 5206, 4983...</td>\n",
              "      <td>[[0, {35}], [15, {97, 74, 106}], [17, {712, 72...</td>\n",
              "      <td>[[0, {36}], [15, {3490, 4213}], [59, {18}], [7...</td>\n",
              "      <td>[[0, {37}], [1, {411}], [4, {414}], [15, {3715...</td>\n",
              "      <td>[[0, {38}], [1, {428}], [5, {124}], [7, {32, 1...</td>\n",
              "      <td>[[0, {39}], [15, {36}], [23, {36}], [25, {25}]...</td>\n",
              "      <td>[[0, {40}], [15, {4067, 37, 3889, 2482, 1234, ...</td>\n",
              "      <td>[[0, {41}]]</td>\n",
              "      <td>[[0, {97, 66, 452, 837, 231, 44, 556, 147, 53,...</td>\n",
              "      <td>[[0, {98, 1527, 356, 440, 1510, 776, 330, 491,...</td>\n",
              "      <td>[[0, {46}], [6, {108}], [12, {29}], [24, {912,...</td>\n",
              "      <td>[[0, {47}], [15, {2049, 3713, 2443, 3214, 2327...</td>\n",
              "      <td>[[0, {49}], [15, {5283}], [25, {700}], [29, {4...</td>\n",
              "      <td>...</td>\n",
              "      <td>[[1130, {240}]]</td>\n",
              "      <td>[[1130, {262}]]</td>\n",
              "      <td>[[1130, {499}]]</td>\n",
              "      <td>[[1130, {512}]]</td>\n",
              "      <td>[[1131, {0}]]</td>\n",
              "      <td>[[1131, {1}]]</td>\n",
              "      <td>[[1131, {9}]]</td>\n",
              "      <td>[[1131, {10}]]</td>\n",
              "      <td>[[1131, {24}]]</td>\n",
              "      <td>[[1131, {28}]]</td>\n",
              "      <td>[[1131, {182}]]</td>\n",
              "      <td>[[1131, {274, 275}]]</td>\n",
              "      <td>[[1131, {289}]]</td>\n",
              "      <td>[[1131, {313}]]</td>\n",
              "      <td>[[1131, {318}]]</td>\n",
              "      <td>[[1131, {339}]]</td>\n",
              "      <td>[[1131, {342}]]</td>\n",
              "      <td>[[1131, {344}]]</td>\n",
              "      <td>[[1131, {345}]]</td>\n",
              "      <td>[[1131, {347}]]</td>\n",
              "      <td>[[1131, {374}]]</td>\n",
              "      <td>[[1131, {427}]]</td>\n",
              "      <td>[[1131, {443}]]</td>\n",
              "      <td>[[1131, {476}]]</td>\n",
              "      <td>[[1131, {572}]]</td>\n",
              "      <td>[[1131, {601}]]</td>\n",
              "      <td>[[1131, {604}]]</td>\n",
              "      <td>[[1131, {614}]]</td>\n",
              "      <td>[[1131, {672}]]</td>\n",
              "      <td>[[1131, {727}]]</td>\n",
              "      <td>[[1131, {748}]]</td>\n",
              "      <td>[[1131, {757}]]</td>\n",
              "      <td>[[1132, {50}]]</td>\n",
              "      <td>[[1132, {267, 61, 534}]]</td>\n",
              "      <td>[[1132, {268}]]</td>\n",
              "      <td>[[1132, {576}]]</td>\n",
              "      <td>[[1132, {577}]]</td>\n",
              "      <td>[[1132, {578}]]</td>\n",
              "      <td>[[1132, {615}]]</td>\n",
              "      <td>[[1132, {636}]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>145</td>\n",
              "      <td>412</td>\n",
              "      <td>65</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1176</td>\n",
              "      <td>405</td>\n",
              "      <td>567</td>\n",
              "      <td>8326</td>\n",
              "      <td>1222</td>\n",
              "      <td>3</td>\n",
              "      <td>75</td>\n",
              "      <td>500</td>\n",
              "      <td>147</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>172</td>\n",
              "      <td>209</td>\n",
              "      <td>199</td>\n",
              "      <td>1</td>\n",
              "      <td>836</td>\n",
              "      <td>71</td>\n",
              "      <td>1045</td>\n",
              "      <td>171</td>\n",
              "      <td>352</td>\n",
              "      <td>348</td>\n",
              "      <td>189</td>\n",
              "      <td>191</td>\n",
              "      <td>187</td>\n",
              "      <td>406</td>\n",
              "      <td>1</td>\n",
              "      <td>623</td>\n",
              "      <td>591</td>\n",
              "      <td>137</td>\n",
              "      <td>395</td>\n",
              "      <td>40</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows Ã— 71867 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea00b9e5-285a-4eb8-961e-c8d39bdd47d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ea00b9e5-285a-4eb8-961e-c8d39bdd47d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ea00b9e5-285a-4eb8-961e-c8d39bdd47d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                          newsgroups  ...            laver\n",
              "0  [[0, {0, 1416}], [15, {0, 2039}], [23, {0, 232...  ...  [[1132, {636}]]\n",
              "1                                                145  ...                1\n",
              "\n",
              "[2 rows x 71867 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################################   part c   ############################################\n",
        "\n",
        "def check_tokens_in_index(tokens):\n",
        "  for token in tokens:\n",
        "    if token not in positional_index:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "def find_word_positions_in_doc(posting_list, doc):\n",
        "    for a in posting_list:\n",
        "        if a[0] == doc:     \n",
        "            return a[1]            #return set of word positions in doc\n",
        "    return {}\n",
        "\n",
        "def find_doc_position_pair_list(word):\n",
        "    doc_position_pair_list = []\n",
        "    word_postings = positional_index[word][0]\n",
        "    for a in word_postings:\n",
        "        for position in a[1]:      #for each position of doc a[0]\n",
        "            doc_position_pair_list.append((a[0], position))\n",
        "    return doc_position_pair_list\n",
        "\n",
        "def positional_func(first_word_doc_position_set, query_tokens):\n",
        "    matched_docs = []\n",
        "    for a in first_word_doc_position_set:\n",
        "        doc = a[0]\n",
        "        pos = a[1]\n",
        "        token_count = 0\n",
        "\n",
        "        for token in query_tokens:                          #for all words after first word\n",
        "            pos = pos+1                                     #to check if next word is on next position\n",
        "            token_posting = positional_index[token][0]      #posting list of next word\n",
        "            token_docs = [a[0] for a in token_posting]      #docs list of next word\n",
        "            if doc in token_docs:                           #if same doc as of first word in next word also\n",
        "                doc_positions = find_word_positions_in_doc(token_posting, doc)\n",
        "                if pos in doc_positions:\n",
        "                    token_count += 1\n",
        "                else:\n",
        "                    token_count += 1\n",
        "                    break\n",
        "            if token_count == len(query_tokens):\n",
        "                matched_docs.append(a[0])\n",
        "\n",
        "    return set(matched_docs)\n",
        "\n",
        "def starter_method():\n",
        "    query = input(\"Enter phrase query: \")\n",
        "    if query == \"\":\n",
        "      print(\"Enter valid phrase query!\")\n",
        "      return\n",
        "    query_tokens = preprocess_data(query)\n",
        "    if(check_tokens_in_index(query_tokens)):                    #check only if all token exists in positional index\n",
        "      retrieved_doc_count = 0\n",
        "      retrieved_doc_list = []\n",
        "      if len(query_tokens)==1:                                  #if query has only one word\n",
        "        retrieved_doc_count = len(positional_index[query_tokens[0]][0])\n",
        "        for a in positional_index[query_tokens[0]][0]:\n",
        "          retrieved_doc_list.append(doc_map[a[0]])\n",
        "        print(\"The number of documents retrieved:\",retrieved_doc_count)\n",
        "        print(\"The list of document names retrieved:\",retrieved_doc_list)\n",
        "\n",
        "      else:                                                     #if query has more than one word\n",
        "          first_word = query_tokens[0]\n",
        "          first_word_doc_position_list = find_doc_position_pair_list(first_word)\n",
        "          query_tokens.pop(0)\n",
        "          matched_docs_set = positional_func(first_word_doc_position_list, query_tokens)  #find common docs in first word posting and remaining words\n",
        "          print(\"The number of documents retrieved:\", len(matched_docs_set))\n",
        "          \n",
        "          for doc in matched_docs_set:\n",
        "            retrieved_doc_list.append(doc_map[doc])\n",
        "          print(\"The list of document names retrieved:\",retrieved_doc_list)\n",
        "\n",
        "\n",
        "    else:\n",
        "      print(\"No result available!\")                           \n",
        "  "
      ],
      "metadata": {
        "id": "xY_ZLWcVFD7I"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "starter_method()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29cENRK_Fbub",
        "outputId": "e83e2a01-d264-4af6-f84a-0823f52ac98e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter phrase query: welcome\n",
            "The number of documents retrieved: 94\n",
            "The list of document names retrieved: ['st_silic.txt', 'top10st2.txt', 'top10st1.txt', 'top10.txt', 'texican.dic', 'texican.lex', 'quack26.txt', 'quotes.jok', 'pracjoke.txt', 'renorthr.txt', 'nigel.6', 'nukewar.txt', 'psilaine.hum', 'nysucks.hum', 'passage.hum', 'onetoone.hum', 'onetotwo.hum', 'oldeng.hum', 'dieter.txt', 'dover.poem', 'feggaqui.txt', 'goforth.hum', 'hotel.txt', 'grail.txt', 'insults1.txt', 'jac&tuu.hum', 'insult.lst', 'jokes1.txt', 'humor9.txt', 'letter_f.sch', 'lawyer.jok', 'looser.hum', 'lawskool.txt', 'math.1', 'making_y.wel', 'luvstory.txt', 'luzerzo2.hum', 'misery.hum', 'manners.txt', 'miami.hum', 'adcopy.hum', 'myheart.hum', 'moose.txt', 'boneles2.txt', 'bmdn01.txt', 'bbq.txt', 'dead5.txt', 'conan.txt', 'modest.hum', 'mlverb.hum', 'comrevi1.hum', 'dead2.txt', 'bhb.ill', 'bread.rcp', 'beginn.ers', 'butcher.txt', 'candy.txt', 'get.drunk.cheap', 'homebrew.txt', 'stuf10.txt', 'films_gl.txt', 'bnbeg2.4.txt', 'stuf11.txt', 'crzycred.lst', 'beauty.tm', 'oliver02.txt', 'lost.txt', 'amazing.epi', 'epi_tton.txt', 'aboutada.txt', 'gd_tznew.txt', 'epi_bnb.txt', 'bnbguide.txt', 'twilight.txt', 'soleleer.hum', 'dym', 'english.txt', 'dead4.txt', 'oliver.txt', 'vegan.rcp', 'epikarat.txt', 'snapple.rum', 'drinks.gui', 'horflick.txt', 'cybrtrsh.txt', 'mindvox', 'lipkovits.txt', 'hackmorality.txt', 'truthlsd.hum', 'thievco.txt', 'urban.txt', 'wisconsi.txt', 'xibovac.txt', 'talebeat.hum']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Que2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}