# -*- coding: utf-8 -*-
"""Que2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/119tvqCMxYy0I67tp7PS3-QUiOQHayWpf
"""

import pandas as pd
import numpy as np
import glob
import os
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

from google.colab import drive
drive.mount('/content/drive')

nltk.download('stopwords')  # download stopwords
nltk.download('punkt')
nltk.download('wordnet')

files_path = "/content/drive/MyDrive/Humor,Hist,Media,Food"

#Store path of all files in paths
paths = []
for (dirpath, dirnames, filenames) in os.walk(str(files_path)):
  for i in filenames:
     paths.append(str(dirpath)+str("/")+i)

print(len(paths))

######################################   part a   ############################################

#Convert the text to lower case
def convert_lower_case(text):   
    return np.char.lower(text)

#Perform word tokenization
def word_tokenization(text):
  tokens = word_tokenize(str(text))
  return tokens

#Remove stopwords from tokens
def remove_stop_words(tokens):  
    stop_words = stopwords.words('english')
    filtered_tokens = []
    for word in tokens:
        if word not in stop_words:
            filtered_tokens.append(word)
    return filtered_tokens

#Remove punctuation marks from tokens
def remove_punctuation(words):   
    filtered_words = []
    for word in words:
        filtered_word = re.sub(r'[^\w\s]', ' ', word)    #\w for string containing char a-z,A-Z,0-9,_, \s for whitespaces, ^for except these 
        if filtered_word != '':
            filtered_words.append(filtered_word)
    return filtered_words

#Remove blank space tokens
def remove_blank_space_tokens(words):
  words=' '.join(words).split()
  return words 

#Perform preprocessing
def preprocess_data(text):
    text = convert_lower_case(text)
    tokens = word_tokenization(text)
    filtered_tokens = remove_stop_words(tokens)
    filtered_tokens = remove_punctuation(filtered_tokens)
    filtered_tokens = remove_blank_space_tokens(filtered_tokens)
    return filtered_tokens

######################################   part b   ############################################

def find_pos_of_list_having_doc(posting_list, doc):
    pos = 0
    for a in posting_list:
        if a[0] == doc:     
            return pos         #return position of list in postings having doc at 0th position
        pos+=1
    return ""

doc_no = 0
positional_index = {}
doc_map = {}

for path in paths:
    file = open(path, 'r', encoding='ISO-8859-1')
    text = file.read().strip()
    file.close()
    tokens = preprocess_data(text)
 
    for position, term in enumerate(tokens):
        if term in positional_index : 
            positional_index[term][1] += 1
            posting = positional_index[term][0]                    #posting list of token  
            pos = find_pos_of_list_having_doc(posting,doc_no)   
            if pos != "" :                                         #if doc already added
                positional_index[term][0][pos][1].add(position)    #add position to set
            else:                                                  #if doc added first time 
                positional_index[term][0].append([])               #create new list 
                positional_index[term][0][-1].append(doc_no)       #append doc_no to the newly appended list(-1 is position of new list)
                positional_index[term][0][-1].append(set())        #append set for positions to the newly appended list
                positional_index[term][0][-1][1].add(position)     #add position to this newly appended set
        else:
            positional_index[term] = []                            #new list as a value for token
            positional_index[term].append([])                      #list at 1st pos for storing posting lists for each doc
            positional_index[term].append(1)                       #occurence count of token at 2nd position
            positional_index[term][0].append([])                   #create new list 
            positional_index[term][0][-1].append(doc_no)           #append doc_no to the newly appended list(-1 is position of new list)
            positional_index[term][0][-1].append(set())            #append set for positions to the newly appended list
            positional_index[term][0][-1][1].add(position)         #add position to this newly appended set
    filename = os.path.basename(path)
    doc_map[doc_no] = filename
    doc_no+=1
    
df = pd.DataFrame(positional_index)
display(df)

######################################   part c   ############################################

def check_tokens_in_index(tokens):
  for token in tokens:
    if token not in positional_index:
      return False
  return True

def find_word_positions_in_doc(posting_list, doc):
    for a in posting_list:
        if a[0] == doc:     
            return a[1]            #return set of word positions in doc
    return {}

def find_doc_position_pair_list(word):
    doc_position_pair_list = []
    word_postings = positional_index[word][0]
    for a in word_postings:
        for position in a[1]:      #for each position of doc a[0]
            doc_position_pair_list.append((a[0], position))
    return doc_position_pair_list

def positional_func(first_word_doc_position_set, query_tokens):
    matched_docs = []
    for a in first_word_doc_position_set:
        doc = a[0]
        pos = a[1]
        token_count = 0

        for token in query_tokens:                          #for all words after first word
            pos = pos+1                                     #to check if next word is on next position
            token_posting = positional_index[token][0]      #posting list of next word
            token_docs = [a[0] for a in token_posting]      #docs list of next word
            if doc in token_docs:                           #if same doc as of first word in next word also
                doc_positions = find_word_positions_in_doc(token_posting, doc)
                if pos in doc_positions:
                    token_count += 1
                else:
                    token_count += 1
                    break
            if token_count == len(query_tokens):
                matched_docs.append(a[0])

    return set(matched_docs)

def starter_method():
    query = input("Enter phrase query: ")
    if query == "":
      print("Enter valid phrase query!")
      return
    query_tokens = preprocess_data(query)
    if(check_tokens_in_index(query_tokens)):                    #check only if all token exists in positional index
      retrieved_doc_count = 0
      retrieved_doc_list = []
      if len(query_tokens)==1:                                  #if query has only one word
        retrieved_doc_count = len(positional_index[query_tokens[0]][0])
        for a in positional_index[query_tokens[0]][0]:
          retrieved_doc_list.append(doc_map[a[0]])
        print("The number of documents retrieved:",retrieved_doc_count)
        print("The list of document names retrieved:",retrieved_doc_list)

      else:                                                     #if query has more than one word
          first_word = query_tokens[0]
          first_word_doc_position_list = find_doc_position_pair_list(first_word)
          query_tokens.pop(0)
          matched_docs_set = positional_func(first_word_doc_position_list, query_tokens)  #find common docs in first word posting and remaining words
          print("The number of documents retrieved:", len(matched_docs_set))
          
          for doc in matched_docs_set:
            retrieved_doc_list.append(doc_map[doc])
          print("The list of document names retrieved:",retrieved_doc_list)


    else:
      print("No result available!")

starter_method()